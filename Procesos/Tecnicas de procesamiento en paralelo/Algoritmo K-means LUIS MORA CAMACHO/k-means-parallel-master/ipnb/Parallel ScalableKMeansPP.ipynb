{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\IPython\\parallel.py:13: ShimWarning: The `IPython.parallel` package has been deprecated since IPython 4.0. You should import from ipyparallel instead.\n",
      "  \"You should import from ipyparallel instead.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from IPython import parallel\n",
    "import KMeansBase\n",
    "import KMeansPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing numpy on engine(s)\n"
     ]
    }
   ],
   "source": [
    "rc = parallel.Client()\n",
    "# load balanced view\n",
    "d_view = rc[:]\n",
    "# do synchronized processing\n",
    "d_view.block=True\n",
    "\n",
    "# import Numpy\n",
    "with d_view.sync_imports():\n",
    "    import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_centroids(data, k):\n",
    "    ''' select k random data points from the data'''\n",
    "    return data[numpy.random.choice(range(data.shape[0]), k, replace=False)]\n",
    "\n",
    "def compare_centroids(old_centroids, new_centroids, precision=-1):\n",
    "    if precision == -1:\n",
    "        return numpy.array_equal(old_centroids, new_centroids)\n",
    "    else:\n",
    "        diff = numpy.sum((new_centroids - old_centroids)**2, axis=1)\n",
    "        if numpy.max(diff) <= precision:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "def lloyds_iteration(data, centroids):\n",
    "    # find the Euclidean distance between a center and a data point\n",
    "    # centroids array shape = k x m\n",
    "    # data array shape = n x m\n",
    "    # In order to broadcast it, we have to introduce a third dimension into data\n",
    "    # data array becomes n x 1 x m\n",
    "    # now as a result of broadcasting, both array sizes will be n x k x m\n",
    "    data_ex = data[:, numpy.newaxis, :]\n",
    "    euclidean_dist = (data_ex - centroids) ** 2\n",
    "    # now take the summation of all distances along the 3rd axis(length of the dimension is m).\n",
    "    # This will be the total distance from each centroid for each data point.\n",
    "    # resulting vector will be of size n x k\n",
    "    distance_arr = numpy.sum(euclidean_dist, axis=2)\n",
    "\n",
    "    # now we need to find out to which cluster each data point belongs.\n",
    "    # Use a matrix of n x k where [i,j] = 1 if the ith data point belongs\n",
    "    # to cluster j.\n",
    "    min_location = numpy.zeros(distance_arr.shape)\n",
    "    min_location[range(distance_arr.shape[0]), numpy.argmin(distance_arr, axis=1)] = 1\n",
    "\n",
    "    # calculate J\n",
    "    j_val = numpy.sum(distance_arr[min_location == True])\n",
    "\n",
    "    # calculates the new centroids\n",
    "    new_centroids = numpy.empty(centroids.shape)\n",
    "    membership_count = numpy.empty(centroids.shape[0])\n",
    "    for col in range(0, centroids.shape[0]):\n",
    "        new_centroids[col] = numpy.mean(data[min_location[:, col] == True, :], axis=0)\n",
    "        membership_count[col] = numpy.count_nonzero(min_location[:, col])\n",
    "        \n",
    "    return {'j-value':j_val, 'centroids':new_centroids, 'element-count':membership_count}\n",
    "\n",
    "def ScalableKMeansPP(data, l, centroids):\n",
    "    data_ex = data[:, numpy.newaxis, :]\n",
    "    euclidean_dist = (data_ex - centroids) ** 2\n",
    "    distance_arr = numpy.sum(euclidean_dist, axis=2)\n",
    "    # find the minimum distance, this will be the weight\n",
    "    min = numpy.min(distance_arr, axis=1).reshape(-1, 1)\n",
    "    # let's use weighted reservoir sampling algorithm to select l centroids\n",
    "    random_numbers = numpy.random.rand(min.shape[0], min.shape[1])\n",
    "    # replace zeros in min if available with the lowest positive float in Python\n",
    "    min[numpy.where(min==0)] = numpy.nextafter(0,1)\n",
    "    # take the n^th root of random numbers where n is the weights\n",
    "    with numpy.errstate(all='ignore'):\n",
    "        random_numbers = random_numbers ** (1.0/min)\n",
    "    # pick the highest l\n",
    "    cent = data[numpy.argsort(random_numbers, axis=0)[:, 0]][::-1][:l, :]\n",
    "    # combine the new set of centroids with the previous set\n",
    "    centroids = numpy.vstack((centroids, cent))\n",
    "    # now we have the initial set of centroids which is higher than k.\n",
    "    # we should reduce this to k using scalable K-Means++\n",
    "    euclidean_dist = (data_ex - centroids) ** 2\n",
    "    distance_arr = numpy.sum(euclidean_dist, axis=2)\n",
    "    min_location = numpy.zeros(distance_arr.shape)\n",
    "    min_location[range(distance_arr.shape[0]), numpy.argmin(distance_arr, axis=1)] = 1\n",
    "    weights = numpy.array([numpy.count_nonzero(min_location[:, col]) for col in range(centroids.shape[0])]).reshape(-1,1)\n",
    "    return {'centroids': centroids, 'weights': weights}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centroids from one engine:  (3, 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities are not non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b387ac426e2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weights'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mkmeans_pp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeansPP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKMeansPP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_locations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmeans_pp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;31m# calculates the new centroids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mnew_centroids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\k-means-parallel-master\\ipnb\\KMeansBase.py\u001b[0m in \u001b[0;36mcluster\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lloyds_iterations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_initial_centroids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\k-means-parallel-master\\ipnb\\KMeansBase.py\u001b[0m in \u001b[0;36m_lloyds_iterations\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_lloyds_iterations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m#warnings.simplefilter(\"error\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mcentroids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_centroids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;31m#print('Initial Centroids:', centroids)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\k-means-parallel-master\\ipnb\\KMeansPP.py\u001b[0m in \u001b[0;36m_initial_centroids\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mprob_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mj_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;31m# select the next centroid using the probability distribution calculated before\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mcentroids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprob_dist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: probabilities are not non-negative"
     ]
    }
   ],
   "source": [
    "data = numpy.random.randn(1000000,2)\n",
    "# distribute the data among the engines\n",
    "d_view.scatter('data', data)\n",
    "\n",
    "# first pick a random centroid. Ask one engine to pick the first random centroid\n",
    "centroids = rc[0].apply(initial_centroids, parallel.Reference('data'),1).get()\n",
    "\n",
    "r = 3\n",
    "l = 2\n",
    "k = 4\n",
    "passes = 0\n",
    "while passes < r:\n",
    "    result = d_view.apply(ScalableKMeansPP, parallel.Reference('data'), l, centroids)\n",
    "    print('centroids from one engine: ', result[0]['centroids'].shape)\n",
    "    # combine the centroids for the next iteration\n",
    "    centroids = numpy.vstack(r['centroids'] for r in result)\n",
    "    passes += 1\n",
    "# next step is to calculate k centroids out of the centroids returned by each engine\n",
    "# for this we use KMeans++\n",
    "    weights = numpy.vstack(r['weights'] for r in result)\n",
    "    kmeans_pp = KMeansPP.KMeansPP(weights, k)\n",
    "    _, _, _, min_locations = kmeans_pp.cluster()\n",
    "# calculates the new centroids\n",
    "new_centroids = numpy.empty((k, data.shape[1]))\n",
    "for col in range(0, k):\n",
    "    new_centroids[col] = numpy.mean(centroids[min_locations[:, col] == True, :], axis=0)\n",
    "\n",
    "centroids = new_centroids\n",
    "# now do the lloyd's iterations\n",
    "stabilized = False\n",
    "iterations = 0\n",
    "while not stabilized:\n",
    "    iterations += 1\n",
    "    ret_vals = d_view.apply(lloyds_iteration, parallel.Reference('data'), centroids)\n",
    "    member_count = numpy.sum(numpy.array([r['element-count'] for r in ret_vals]).reshape(len(ret_vals),-1),axis=0)\n",
    "    local_sum = numpy.array([r['centroids'] * r['element-count'].reshape(-1,1) for r in ret_vals])\n",
    "    new_centroids = numpy.sum(local_sum, axis=0)/member_count.reshape(-1,1)\n",
    "    if compare_centroids(centroids, new_centroids):\n",
    "        stabilized = True\n",
    "    else:\n",
    "        centroids = new_centroids\n",
    "\n",
    "print('Iterations:', iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
