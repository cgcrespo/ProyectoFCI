{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython import parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = parallel.Client()\n",
    "# load balanced view\n",
    "d_view = rc[:]\n",
    "# do synchronized processing\n",
    "d_view.block=True\n",
    "\n",
    "# import Numpy\n",
    "with d_view.sync_imports():\n",
    "    import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_centroids(data, k):\n",
    "    ''' select k random data points from the data'''\n",
    "    return data[numpy.random.choice(range(data.shape[0]), k, replace=False)]\n",
    "\n",
    "def compare_centroids(old_centroids, new_centroids, precision=-1):\n",
    "    if precision == -1:\n",
    "        return np.array_equal(old_centroids, new_centroids)\n",
    "    else:\n",
    "        diff = np.sum((new_centroids - old_centroids)**2, axis=1)\n",
    "        if np.max(diff) <= precision:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "def lloyds_iteration(data, centroids):\n",
    "    # find the Euclidean distance between a center and a data point\n",
    "    # centroids array shape = k x m\n",
    "    # data array shape = n x m\n",
    "    # In order to broadcast it, we have to introduce a third dimension into data\n",
    "    # data array becomes n x 1 x m\n",
    "    # now as a result of broadcasting, both array sizes will be n x k x m\n",
    "    data_ex = data[:, numpy.newaxis, :]\n",
    "    euclidean_dist = (data_ex - centroids) ** 2\n",
    "    # now take the summation of all distances along the 3rd axis(length of the dimension is m).\n",
    "    # This will be the total distance from each centroid for each data point.\n",
    "    # resulting vector will be of size n x k\n",
    "    distance_arr = numpy.sum(euclidean_dist, axis=2)\n",
    "\n",
    "    # now we need to find out to which cluster each data point belongs.\n",
    "    # Use a matrix of n x k where [i,j] = 1 if the ith data point belongs\n",
    "    # to cluster j.\n",
    "    min_location = numpy.zeros(distance_arr.shape)\n",
    "    min_location[range(distance_arr.shape[0]), numpy.argmin(distance_arr, axis=1)] = 1\n",
    "\n",
    "    # calculate J\n",
    "    j_val = numpy.sum(distance_arr[min_location == True])\n",
    "\n",
    "    # calculates the new centroids\n",
    "    new_centroids = numpy.empty(centroids.shape)\n",
    "    membership_count = numpy.empty(centroids.shape[0])\n",
    "    for col in range(0, centroids.shape[0]):\n",
    "        new_centroids[col] = numpy.mean(data[min_location[:, col] == True, :], axis=0)\n",
    "        membership_count[col] = numpy.count_nonzero(min_location[:, col])\n",
    "        \n",
    "    return {'j-value':j_val, 'centroids':new_centroids, 'element-count':membership_count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd_view' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a1fb6404d393>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# distribute the data among the engines\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0md_view\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mcentroids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_view\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_centroids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparallel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mstabilized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'd_view' is not defined"
     ]
    }
   ],
   "source": [
    "data = np.random.randn(1000000,2)\n",
    "# distribute the data among the engines\n",
    "d_view.scatter('data', data)\n",
    "centroids = np.array(d_view.apply(initial_centroids, parallel.Reference('data'),2)).reshape(4,2)\n",
    "stabilized = False\n",
    "iterations = 0\n",
    "while not stabilized:\n",
    "    iterations += 1\n",
    "    ret_vals = d_view.apply(lloyds_iteration, parallel.Reference('data'), centroids)\n",
    "    member_count = np.sum(np.array([r['element-count'] for r in ret_vals]).reshape(len(ret_vals),-1),axis=0)\n",
    "    local_sum = np.array([r['centroids'] * r['element-count'].reshape(-1,1) for r in ret_vals])\n",
    "    new_centroids = np.sum(local_sum, axis=0)/member_count.reshape(-1,1)\n",
    "    if compare_centroids(centroids, new_centroids):\n",
    "        stabilized = True\n",
    "    else:\n",
    "        centroids = new_centroids\n",
    "\n",
    "print('Iterations:', iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
